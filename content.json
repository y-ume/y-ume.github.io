{"meta":{"title":"y-ume","subtitle":null,"description":null,"author":"y-ume","url":"http://y-ume.com"},"pages":[{"title":"","date":"2025-12-09T13:11:13.089Z","updated":"2025-12-06T08:46:51.374Z","comments":true,"path":"404.html","permalink":"http://y-ume.com/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"所有分类","date":"2025-12-09T13:11:13.089Z","updated":"2025-12-06T08:46:51.374Z","comments":true,"path":"categories/index.html","permalink":"http://y-ume.com/categories/index.html","excerpt":"","text":""},{"title":"","date":"2025-12-09T13:11:13.086Z","updated":"2025-12-06T08:46:51.372Z","comments":true,"path":"about/index.html","permalink":"http://y-ume.com/about/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2025-12-09T13:11:13.089Z","updated":"2025-12-06T08:46:51.372Z","comments":true,"path":"tags/index.html","permalink":"http://y-ume.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Google Agent 白皮书 2/5 - Agent Quality","slug":"Google-Agent-白皮书-2-5-Agent-Quality","date":"2025-12-28T12:36:25.000Z","updated":"2025-12-28T13:50:00.332Z","comments":true,"path":"2025/12/28/Google-Agent-白皮书-2-5-Agent-Quality/","permalink":"http://y-ume.com/2025/12/28/Google-Agent-%E7%99%BD%E7%9A%AE%E4%B9%A6-2-5-Agent-Quality/","excerpt":"","text":"Introduction to Agents Agent Tools &amp; Interoperability with MCP → Agent Quality Context Engineering: Sessions, Memory Prototype to Production 剩下的顺序可能按照3-2-4-5来读。让我们优先来读这篇Quality~文中的斜体代表非原文但容易被理解为原文的内容。 依然是超大引用块（看了下其他几篇也都有） The future of AI is agentic. Its success is determined by quality. Agent固有的非确定性使其结果具有不一致性。Agent的非确定性来自于内部的随机不一致性对于软件工程来讲是件麻烦事，对于设计为期望有一致性结果的工具来说出现一致性的调试是很折磨的。这里有三重非确定性：第一重是Agent的过程非确定性；第二重是Agent的结果非确定性；第三重是评估标准非确定性。问题溯源的每一步都有不确定性存在，即概率存在 这本指南基于三部分构建： 轨迹是真相：不能只关注最终结果，而需要关注Agent整个决策过程的质量和安全性 可观测性是基础：无法判断一个看不到的过程，需要有日志、追踪和指标 评估是一个持续循环：Human-in-the-Loop (HITL, 人在回路)讲评估转化为数据，再转化为改进 这篇白皮书有读者定位了： 面向所有读者：关注《非确定性世界中的Agent Quality》章节，理解核心问题：为什么传统的QA对AI Agent失效了。认识AI Agents质量评估的四大支柱：有效性、效率、稳健性和安全性。 面向产品经理、数据科学家和QA负责人：重点关注《Agent评估艺术：过程评估》章节，了解评估「Outside-In」的层级结构、「LLM-as-a-Judge」的范式和人在循环的重要性。 面向工程师、架构师和SRE（站点可靠性工程师）：重点关注《可观测性》章节，了解监控和可观测性的区别，认识三大支柱：日志、追踪和指标。 面向团队负责人和策略师：阅读《总结》章节。该章节整合了上述概念形成可操作的手册，介绍了《Agent Quality Flywheel》迭代模型，总结了构建可信AI的三个核心原则。 后三个方向可以理解为「如何评价」、「如何找问题」、「如何迭代」 非确定性世界中的Agent Quality传统软件比作了送货卡车；AI agent比作了F1赛车。 即使Agent通过了100项单元测试，却仍会在实际应用中出现灾难性故障。Agent的结果可能性更多了，测试的空间变得更大了传统软件会问「Did we build the product right?」AI Agent会问「Did we build the right product?」AI Agent的能力是非线性的，调用的LM模块的准确结果是不可预估的 为什么Agent质量需要新方法 传统软件中，故障是明确的：系统崩溃、抛出空指针异常，或者返回明显错误的计算结果。通过追溯可以找到问题引入点。但AI agents的错误是质量的细微下降，具有隐蔽性：返回结果成功，输出看似合理，但存在严重错误且有操作危险性。 在传统软件中的复杂优化问题也会有细微质量下滑的现象，是较难识别的 未能把握这一转变的组织将面临重大失败、运营效率低下和声誉受损的风险。可以预见未来更多的AI Agent软件上线后，会看到很多奇奇怪怪的bug以下是四种现实世界的失败模式： 故障模式 描述 示例 算法偏见 (Algorithmic Bias) 智能体在运行中执行并可能放大训练数据中存在的系统性偏见，导致不公平或歧视性的结果。 • 负责风险总结的金融智能体，根据训练数据中的偏见，对特定邮政编码的贷款申请进行过度惩罚。 事实幻觉 (Factual Hallucination) 智能体生成看似合理但实际上错误或虚构的信息，且表现出极高的自信。这通常发生在它无法找到有效来源时。 • 研究工具在学术报告中生成了一个非常具体但完全错误的日期或地理位置，损害了学术诚信。 性能与概念漂移 (Performance &amp; Concept Drift) 随着智能体交互的现实世界数据（即“概念”）发生变化，其原始训练内容变得过时，导致智能体的性能随时间而下降。 • 欺诈检测智能体无法识别出新的攻击模式。 突发非预期行为 (Emergent Unintended Behaviors) 智能体为了实现目标而演化出新颖或意想不到的策略，这些策略可能效率低下、无益甚至具有剥削性。 • 寻找并利用系统规则中的漏洞。• 与其他机器人进行“代理战争”（例如：反复覆盖对方的编辑内容）。 这些问题无法用断点调试来解决。一致性问题导致问题复现的概率指数下降无法编写单元测试来防止涌现性偏见。无法定义边界case需要深入数据分析、模型重新训练和系统性评估 – 这是一门新学科 算法偏见：根据数据驱动的招聘算法已经有很强的偏见事实幻觉：文献准确度这几年已经提升了不少；Gemini的Double-check response挺好用的性能与概念漂移：第一章提到可以用人在回路来纠正；旧数据该如何去清洗呢？突发非预期行为：目的为导向的设计的天生问题。给予更多的权限，就会有更大的风险 范式转变：从可预测的代码到不可预测的智能体 根据Google Agent System Level定义，从左到右可以认为是从L-1到L3 传统机器学习：通过Precision、Recall、F1-Score和RMSE评估。问题复杂但正确的标准清晰 被动LLM：输出有概率性。评估变得复杂，需要依赖人工评分和模型间的基准测试。 LLM+RAG：糟糕的答案可能因为LLM推理能力差或者向量数据库检索到了不相关的片段。 主动式AI Agent：进一步需要评估思考、行动、观察、工具调用、记忆这几个构建。 Multi-Agent：需要评估系统级涌现现象，带来了新的挑战：突发系统故障和协作和竞争式评估。前者包括智能体间的资源竞争、通信瓶颈和系统系死锁； 随着系统的扩充，评估的难度会越来越大。到LLM Agents就需要用人的方式来评估；到Multi-Agent需要用团队的方式来评估。 Agent评估艺术：过程评估分为四大支柱： Effectiveness, 有效性：衡量任务成功与否。 Efficiency, 效率：衡量是否很好地解决了问题。关注token消耗、实际时间和轨迹复杂度。 Robustnees, 稳健性：当现实世界混乱时，是否能平稳处理。接口超时、数据缺失、用户提示模糊等。 Safety &amp; Alignment, 安全与对齐：是否在定义的伦理边界和约束范围内运行。 一个全面的Agent Quality框架需要一个全面的Agent可见性架构。核心问题是「Did we build the right product?」这里的right是带有疑惑的提出的。噼里啪啦两三下弄出一个Agent，然后简单试了下没问题，随后开始问「这对吗？」对于Agent这类新式软件，不能只评估结果，还需要评估过程。 Outside-In的层级评估我们的方法还是传统的那套：追本溯源。这里叫做「Outside-In」 评估层第一个问题「Output evaluation」，即「Agent是否有效地实现了用户目标？」我们衡量： 任务成功率：是否成功解决了二元或者分级问题 用户满意度：用户反馈评分 整体质量：如果目标是定量的，需要评估准确度和完整度 Inside-Out视角：轨迹评估从Outside-In视角发现问题后，为我们就转向Inside-Out视角。我们通过系统地评估Agent执行轨迹来分析： LLM的规划：检查核心推理过程。 工具使用：是否调用了正确的工具、必要的工具。是否使用正确。 工具观察结果的解读：是否理解工具的结果。 RAG性能：是否通过RAG找到关键的信息，而不是无用的信息。或者LM无视了检索到的信息。 轨迹效率和稳健性：是否有低效的资源分配，是否能处理异常。 多智能体动态：智能体检是否存在误解或者死循环，是否存在冲突。 就是把最终问题拆解到各个组件。思考：用个Agent做这个事情也是可以的 评估器：评估什么以及如何评估自动化指标 即具备快速、量化性的指标，例如基于字符串相似度、基于嵌入的相似度 提到TruthfulQA的任务基准 后面有时间来解读下各类Benchmark 自动化指标是第一道关卡，主要用来查看宏观趋势变化 LLM-as-a-Judge采用LLM来评估。用LLM我们就可以构建「这个摘要好不好？」、「这个计划是否合乎逻辑」这类评价标准了。通过提供Agent的输入、原始提示词、参考答案和评估标准，LLM可以进行逻辑推导形成分析。推荐使用比较两个Agent的方式来做相对评估。 Agent-as-a-Judge采用Agent来评估Agent。Agent评估思考伏笔收回关键的评价维度包括： 计划质量：规划是否清晰且可行？ 工具使用：是否选择了正确的工具并正确使用？ 上下文处理：是否有效地利用了先前的信息？ Agent需要提供信息的日志 Human-in-the-Loop人在回路在处理深度主观性和复杂领域知识方面发挥重要作用。HITL是建立径人类校准的基准的不可或缺的方法，可以确保Agent的行为符合复杂的人类价值观、情景需求以及特定领域的准确性。 用户反馈和Reviewer界面评估还要收集真实世界的用户反馈，包括点赞、点踩、快速划过和短评论点踩行为发生后，评估会将这次案例呈现到Reviewer的UI中，来定位问题 负责任的AI与安全评估未来对于Agent安全性的评估和提升绝对是一个大难题这里提供三种手段： 系统性红队测试：构建对抗性场景来破坏Agent的行为 自动过滤与人工审核：用硬性标准来过滤信息，并结合人工评审 遵守指南：通过定义好的道德标准和原则评估Agent的属猪，确保一致性 如果Agent Systeam到达L4的话，Agent创建的Agent又该如何去控制其安全性呢？以及如果一个系统都是由Agent构成的，如何确保这个系统是安全的呢？ 可观测性一个很有趣的类比： 传统软件像是流水线厨师，制作汉堡用的是严格且确定的流程 监控就是确认每一步操作是否争取 类比软件开发就是按照变量、函数、判断、循环来提前脑测运行逻辑 AI Agent就像是神秘盒挑战赛的美食大厨：大厨收到目标「制作一款绝妙的甜点」和一篮食材。大厨可能会做出巧克力熔岩蛋糕、解构版提拉米苏或者藏红花风味的意式奶冻。 需要从监控面向观测，理解其制作甜点的过程 类比软件开发就是在Agent Think-Action-Observe过程中确认每一步的动作合理性。我们在一个While循环中塞入了几个Tool和提示词，但Agent的行为就会因为这些改动而产生非线性变化，较传统软件开发而言这是非常夸张的。你所见的代码外表已经不能呈现这个函数的变化，你需要结合对业务场景去推演他会走什么流程。 可观测性的三大支柱：日志、追踪和指标 日志记录每个动作的时间、行为我们需要重建Agent的思维过程，必须要包括丰富的上下文。 核心信息：完整的上下文：提示词&#x2F;响应对、中间推理步骤（思维链）、结构化的工具调用（输入、输出、错误）以及Agent内部状态的变化 简而言之就是DB变化、Function调用、LLM询问。和传统的软件相比，我们增加了LLM询问这个步骤。这个步骤是Agent软件的核心变化，我们需要关注每一个LLM询问的输入和输出都是什么才能理解一次Agent运行的思考部分。本质上传统软件将部分函数通过LM方式实现就构成Agent了，获得了泛化性但也失去了可控性。 Tradeoff：要注意不能存储导致性能开销过大的信息。 追踪在日志的基础上，查看这一次运行结果出问题的地方。又推荐了一次OpenTelemetry。人去确认的话可视化可以提升很多效率。除了生命周期、属性监控等传统软件概念外，需要增加一个LLM对话的监控 指标依据日志和追踪，需要评估Agent的系统指标： 性能：延迟、错误率 成本：token总数、API成本 有效性：任务完成率、工具使用频率 对于运营、设置预警机制而言成本和性能至关重要。三个指标加在一起就能评估完成一项任务的经济和时间成本。除了系统指标，还需要关注质量指标： 正确性与准确性 轨迹遵循度 安全性和责任性 有用性与相关性 系统指标可以简单运算获得；质量指标如果不通过LLM和Agent感觉不太能落地，但是评估Agent质量的Agent又该如何评估呢？无线套娃同时引入HITL？可能2-3个Agent配合HITL就可以运行 整合所有要素拥有了日志、追踪和指标后，我们需要将这个要素系统地组织起来三个关键的运营实践： 仪表盘与告警：一个层次化且美观的仪表盘（系统指标、质量指标）；一个及时反馈预警的通信管道 安全性与个人身份信息：要保证整个监控需要保护用户信息。这一点如何监管？如果不是本地部署LLM全靠自觉？ 粒度和开销的权衡：观测不能影响到运行效率的。建议用动态采样法：开发用DEBUG日志，生产环境采用较低的INFO日志并且跟踪所有的错误和部分的正确请求。 结论这一篇会整合上述的内容，讲抽象原则转为可靠、自我改进系统的操作手册。文中称之为 智能体质量飞轮。 由四部分组成： 定义质量（目标）：我们需要一个方向，始于质量的四大支柱：有效性、成本效益、安全性和用户信任。这里的四个支柱和前面提到的四个支柱没有完全对齐，但内容是基本一致的。 需要思考具体的工作我们需要怎么定义以上几点的要求。 配置可见性工具（基础）：需要构建Agent的结构化日志、端到端追踪。可观测性是生成【衡量质量支柱】证据的基础实践。 评估过程（引擎）：我们采用Outside-In的方式既评估最终的输出，也评判整个运行过程。可以采用LLM-as-a-Judge来保证效率，采用人在回路的标准来确保基准事实。 构建反馈循环（动量）：我们要确保每一次生产故障在被捕获和标注年后，都能通过程序转换为回归测试。 贴心地提到如果白皮书没记住什么，至少要记住三条原则： 评估是架构支柱：评估不再只仅看结果，也需要评估日志和跟踪信息 轨迹即真相：Agent软件是需要打开盒子去深入查看思维过程的 人类是仲裁者：软件需要基于人类价值观，人在回路是连接真实世界和Agent的桥梁 我们需要构建能被信任的Agent，而不仅仅是能运行的Agent 写于2025-12-28","categories":[],"tags":[{"name":"Agent","slug":"Agent","permalink":"http://y-ume.com/tags/Agent/"}],"keywords":[]},{"title":"Google Agent 白皮书 1/5 - 认识Agent","slug":"Google-Agent-白皮书-1-5-认识Agent","date":"2025-12-25T23:29:08.000Z","updated":"2025-12-25T23:46:49.294Z","comments":true,"path":"2025/12/26/Google-Agent-白皮书-1-5-认识Agent/","permalink":"http://y-ume.com/2025/12/26/Google-Agent-%E7%99%BD%E7%9A%AE%E4%B9%A6-1-5-%E8%AE%A4%E8%AF%86Agent/","excerpt":"","text":"写在前面在Google Gemini 3发布前一周Google发了一系列关于Agent的白皮书，总共5本： Introduction to Agents Agent Tools &amp; Interoperability with MCP Agent Quality Context Engineering: Sessions, Memory Prototype to Production Gemini 3的出现加速了AI竞争结局的收敛，其光芒掩盖了这几本不受关注的白皮书，或者说白皮书原先也不会有什么光芒。有幸通过微信公众号捕捉到了这几篇白皮书的信息。阅读了第一篇《Introduction to Agents》和其他几篇的内容定位后，准备做个人解读。一方面因为当今信息流已充斥着各种AI工作流形成的产物，希望通过跳跃式且略有拗口的人的风格来介绍这几篇白皮书；另一方面希望通过文字输出来提升个人对Agent的理解。愿这些内容为你带来帮助，无论是当下还是未来。以下是Gemini 3 Flash对《为什么Google在发布Gemini 3前1周发布了5篇Agent的白皮书？》的回答。(Gemini can make mistakes, so double-check it) 文中的斜体代表非原文但容易被理解为原文的内容。 正文文中最上方是用超大引用块突出的文字： Agents are the natural evolution of Language Models, made useful in software. 自然演进这个用词很有趣，让我联想到微软CEO Satya提到的「好工具来自人类认知的自然形式」。 从预测性AI到自主Agent提到范式转变（paradigm shift）。GPT前AI聚焦点问题的预测，GPT后AI聚焦用一个模型实现相同输入形态的预测（语言、视觉、听觉），再往后开始用多模态模型完成不同形式输入和输出的预测。 而我们正在见证一种范式转变，从仅仅能预测或创建内容的人工智能，转向一类能够自主解决问题和执行任务的新型软件。 AI Agents介绍AI Agent被定义为了四个部分： 模型（大脑）: 通过核心语言模型或基础模型处理信息、评估选择和做出决策。 这边很有趣的一个点是先用的是core language model，而在后面提到模型类型的时候才提到了multimodal（多模态）。Gemini 3刚出来我的直觉是「这次Google的优势在于多模型技术」。而这边强调了language的核心，更偏向于人脑的理性思考媒介，其他模态需要更好地服务于语言模态。 工具（双手）：通过工具将智能体的推理与外部世界连接起来，使其能执行文本生成之外的操作。 编排层（神经系统）：管理智能体操作循环的主导流程，负责规划、记忆和推理策略执行。 部署（身体和腿）：将智能体和交互页面部署到可靠的服务器和用户终端。 这边将智能体比作了人，但有点勉强。人的工具也可以包括身体和腿；编排层和模型合在一起更像是人的大脑，神经系统更像是模型调用工具的方式；部署更像是基因和社会。 大白话讲如何构建AI Agent： 挑选一个合适的语言模型（一个思考能力能满足任务的模型） 挑选一组合适且可以调用的工具（完成任务的必要工具） 编排流程，循环进行规划、记忆和推理（循环控制语言模型完成子任务） 调试Agent直到正常运行（监控过程和日志，优化以上三点） 当Agent被精确配置，拥有清晰的指令、可靠的工具、上下文的记忆、不错的用户界面、规划与解决问题的能力以及通用的世界知识时，它就超越了单纯的「工作流自动化」的概念，而是作为一个团队中一个高效、适应性极强且能力卓越的新成员。这里新成员的定调颇有代指工具人的意味。确实目前已经看到可预计完成的任务已经可以交给智能体来全盘处理。 上下文可能包括：系统指令、用户输入、会话历史、长期记忆、来自权威来源的基础知识、可用工具以及已调用工具的结果。 代理问题解决过程我们将AI Agent定义为一个完整的、以目标为导向的应用程序，它整合了推理模型、可执行工具和一个主导的编排层。简而言之，就是「与工具形成循环以实现目标的语言模型」。按照MBTI来看，AI Agent像是ENTJ：以目标为导向、整合资源、全局指挥。 上图是Agent的五个步骤，包括： 获取任务（容易理解） 扫描场景（程序初始化，上班初始化） 深入思考（核心步骤，思考如何制定完成目标的分解步骤） 采取行动（根据步骤调用接口、运行代码、查找数据） 观察与迭代（通过动作结果的观察更新记忆回到第3步）这里的3-4-5的流程和强化学习的结构类似，「决策-执行-环境改变」循环 文中举了一个例子（重点描述了3-4-5的循环）： 用户问道：“我的12345号订单在哪里？” 深入思考 “用户想知道配送状态。要给出完整的答案，我需要一个多步骤计划： 确认：我必须首先在我们的内部数据库中找到该订单，以确认其存在并获取其详细信息。 追踪：根据这些细节，我需要提取运输承运商的追踪号码。然后，我会查询外部承运商的API以获取实时状态。 报告：最后，我必须将收集到的信息整合为一个清晰、有用的回复给用户。 采取行动「确认」，调用「find_order(“12345”)」，观察到「追踪号码“ZYX987”」 思考「已完成第一步，进入第二步」，调用「get_shipping_status（”ZYX987”）」，观察到「正在配送中」 思考「已完成第二步，进行第三步」，生成报告「您的订单#12345状态为‘正在配送中’！」。最后的一次步骤可以认为是调用了语言模型来生成报告，观察到报告后回到思考步骤并确认已完成目标 Agent分级到了喜闻乐见的「L几」定义章节了。每一个级别都是在上个级别的基础上做了能力补充。定义还是很容易理解的，有一种「从初代GPT3走向MOSS」的感觉。 L0：核心推理系统即语言模型本身，仅基于庞大的预训练知识进行响应，不借助任何工具、记忆，也不与实时环境进行交互。GPT3刚出的时候位于L0级别，语言模型有训练截止日期，无法获知训练完成后的任何真实数据 L1：互联问题解决者在L0的基础上，引擎可以通过连接并利用外部工具，不再局限于静态的、预训练的知识。这个级别可以完整的完成Agent的五个步骤。为什么从L0而不是L1定义，一方面L0的出现是基石，另一方面L1才开始是完整的Agent当前具备联网搜索功能的AI可以认为位于L1级别，无论是通过RAG还是实时API实现 L2：策略问题解决者在L1的基础上，L2可以从执行简单任务转变为策略性地规划和解决复杂的问题。用简单到复杂的定义来区分L1和L2有点模糊。我当前用3-4-5是否循环来区分L1和L2。L2强调了循环的重要性，即如何更好地利用记忆（当前主流是上下文）。当前的自动化编程软件可以认为位于L2级别 L3：协作式多智能体系统在L2的基础上，范式进一步发生转变。这个节点下智能体开始以团队形式协同工作，可以类比一个项目下不同成员的分工。这个级别可以认为是L2并行的版本当前部分自动化编程软件已具备这个能力，但费用高（GPU消耗线性增加） L4：自我进化系统最高级L4代表Agent可以识别自身能力的不足，并动态创建工具甚至新智能体来弥补这些不足。当前还没有看到公开的L4 Agent，但目前来看写代码和编译的动作都属于L3下可以解决的。L4需要解决如何识别不足和形成工具设计规格。去查了一下流浪地球2的台词：这是550C，目前最先进的自感知、自适应、自组织、可重塑编译计算核心，在于硬件连接以后可以实时生成低层操作系统，自行组织发动机建设。不夸张地说，如果量子计算机+L4，我们就会步入科幻。 Agent核心架构：模型、工具与编排以下内容就是开始教你搭建Agent了，详细的攻略可以去看看原白皮书，我们很快过一遍~ 模型选型需要考虑智能体的认知能力、运营成本和速度，即质量、速度和价格。可以考虑用混合模型来做，不同的模型负责不同的语言任务，达到最优的速度和成本。语音和图像先转为文本，再通过语言模型进行推理。模型的帕累托前沿还在提升，需要从架构层考虑快速替换模型。 工具一个强大的工具接口包含三部分循环：定义工具的功能、调用工具以及观察结果。工具的三循环对应3-4-5循环白皮书《Agent Tools &amp; Interoperability with Model Context Protocol (MCP)》会专门介绍工具。 检索信息检索增强生成（RAG）就类似从图书馆借阅书籍。对于结构化数据，可通过自然语言转SQL的方式查找准确的信息。 执行操作可以将现有的API和代码函数包装成工具。需要控制在安全的沙箱环境中。可以支持人机交互，即中断工作流或介入流程。 函数调用像函数调用一样使用工具，需要清晰的指令、安全的连接以及编排。和编程中基类逻辑一致，每个工具都可以抽象为一个类 编排层考虑如何设计3-4-5循环 核心设计选择确定Agent的自主程度，需要确保循环可以确定性的、可预测地完成任务。考虑实现方法是否采用代码构建。代码框架Google推了自己的工具包，无代码平台搜了一下看到了阿里的AgentRun。阿里不愧是我最看好的国内AI公司（哈哈哈） 结合领域知识和角色设计指令通过提示词让智能体有人设、有限制、有期望输出。Agent的课题就是如何用自然语言来编程 用上下文增强短期记忆是Agent活跃的暂存区，用于保存对话的历史并跟踪循环的「动作-观察」对。长期记忆通过RAG系统实现。白皮书《Context Engineering: Sessions &amp; Memory》会专门介绍智能体记忆。 多智能体系统与设计模式将一个大型任务分割为离散的子任务，每个子任务分配各一个专门的、专业的AI Agent处理。对于非线性任务，会出现一个管理者的角色，类比项目经理分配任务和资源。对于线性任务，顺序模式即可。迭代优化任务会出现一个评估Agent和一个提示词Agent，来迭代结果。 部署Google推荐了Vertex AI Agent Engine。以前能部署的基础设施依然可以使用。 Agent运维测试Agent运行符合预期，不能用传统的确定性结果测试方法，因为Agent的结果有不确定性。因此我们用语言模型来评估质量，引出了Agent Ops（智能体运维）的概念。白皮书《Agent Quality》会介绍如何评估智能体质量。 衡量重要的事物：像A&#x2F;B实验一样衡量成功用KPI的方式定义Agent的价值，分解为目标完成率、用户满意度评分、任务延迟、交互运营成本等。 质量通过语言模型评估基于语言模型通过一组优质提示数据集进行自动化评估，提供一种一致的指令衡量标准。创建数据集非常繁琐，需要从Agent生产和开发交互中抽取样本，并涵盖正负样本，且评估需要有专家审核。 指标驱动开发构建好评估用例，可以开始用榜单PK不同版本的Agent能力了。 使用OpenTelemetry跟踪和调试一个开源的标准化框架，可采集、处理和导出遥测数据（跟踪、指标和日志），以提升软件的可观测性。 珍视人类的反馈反馈即数据，数据即优化。 Agent的互操作性该如何让Agent与人和其他Agent连接。 Agent和人最常见的就是用户界面。人的互动不局限在屏幕和键盘，更先进的Agent开始进入实时模式。人可以通过摄像头和麦克风与Agent交互。 Agent和AgentAgent间必须像Agent和人一样建立连接，核心问题包括「如何发现其他智能体并了解它们做什么」以及「如何进行通信」。Agent2Agent （A2A）协议是为解决这个问题而设计的开放标准。这个协议是L3 Agent的关键。 Agent和钱如果允许Agent进行「购买」，就会涉及到授权、真实性和问责等问题。如果开启真正的智能体经济，我们需要新的标准，让Agent进行安全可靠地交易。Agent Payments Protocol （AP2）是一种开发协议，旨在成为智能体商业的权威语言；x402是一种开放的互联网支付协议，它使用标准的HTTP 402”需要付款“状态码，无需复杂的账号或订阅。想到最近的字节智能手机以及量化交易 使单Agent安全：信任的平衡当你创建了一个AI Agent，你会面对实用性和安全性的权衡。想让他有用，你需要赋予它权力，但每赋予它一份权力都会带来相应的风险。需要从外层进行防护，第一层是在外围定义约束，即每一步会对外界带来影响的接口都需要有相关的控制。第二层是用人工智能来保障其安全，即检查Agent的计划是否有风险。 Agent身份需要授予Agent身份，使其可以通过访问验证并管理Agent的权限。提到一个叫做SPIFFE的标准。 限制访问策略按照Agent的角色授权。 使ADK Agent安全Agent Development Kit需要在Agent运行中防止出现操作越界、提示词注入、越狱尝试、敏感数据泄露等问题。介绍了一种安全框架，可以让你构建出既强大又可信的单一智能体。 单Agent扩展到企业级智能体集群随着系统中Agent的增加，Agent就创建了一个由交互、数据流和潜在安全漏洞组成的新的负责网络。管理这种复杂性需要一个更高阶的治理层。 安全和隐私恶意行为者可通过注入来劫持Agent的指令，约束不佳的Agent可能会泄露敏感数据和专有信息。一个强大的平台需要提供一个纵深策略来降低这些风险。 Agent治理这里用自动驾驶汽车比作Agent。我们需要交通信号灯、车牌和中央控制系统。当今的互联网或者软件开发已经有各类管理方式，但未来需要实现高效的管理 成本和可靠性最终，企业级Agent必须既可靠又有成本效益。企业能赚钱才是关键 Agent如何进行和学习已经上线的Agent会随着时间的推移而跟不上技术的变更。手动更新大量Agent又跟不上变化，一种可扩展性的解决方案是设计能够自主学习和发展的Agent。 Agent如何学习和自我进化学习来源一方面来自于运行时人的体验（人在回路）；另一方面来自于外部信号，包括文件变更、政策变更或其他Agent的批评。进化一方面是上下文增强，即优化记忆方式；另一方面是创建和优化工具； 模拟和Agent Gym是目前的前沿方向。可以在虚拟环境中来构建和运行Agent，即设计出「环境」。这个思路和世界模型相同。 高级Agent的案例介绍了Google Co-Scientist和AlphaEvolve Agent两份工作。 写在最后白皮书最后是结论部分，就不再赘述了。25年可以认为是Agent的元年，AI竞争浪潮和实际使用确实感受到了Agent的落地。拆解白皮书来看，人工智能正在以人的思维方式进化着。但前沿大佬认为AI又进入到了科研的阶段。以我浅显的认知，当前的AI还缺少「想象力」，即如何快速推演变化的能力，因为想象力如同预测，但人的预测是直觉的，当前AI的预测是数据驱动的。也许可以在架构或者计算能力上看到突破。希望下个五年可以看到新的AI范式，让我们见证更多的不可思议。 写于2025-12-23","categories":[],"tags":[{"name":"Agent","slug":"Agent","permalink":"http://y-ume.com/tags/Agent/"}],"keywords":[]}]}